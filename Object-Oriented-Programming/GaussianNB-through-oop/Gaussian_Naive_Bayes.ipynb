{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gausian Naive Bayes On Full Dataset, Applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to evaluate the following Probabilities:\n",
    "## 1)\n",
    "\\begin{equation}\n",
    "P(Diagnosis = M | radius mean = x) = P(radius mean = x | Diagnosis = M)\\cdot P(texture mean = y | Diagnosis = M)\\cdot   P(Diagnosis = M)\n",
    "\\end{equation}\n",
    "\n",
    "Now in order to evaluate the likelihood probability **P(radiusmean = x | Diagnosis = M) . P(texturemean = y | Diagnosis = M)** which is given by **Multivariate Joint Gaussian  Distribution PDF**. Now, for this PDF, we need to find out *the best estimate of the parameters of Normal Distribution* because we are assuming that our malignant tumor training data is being sampled from a Normal (gaussian) Distribution. The two parameters will be namely: *mu_hat_m & sigma_hat_m* \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(radiusmean = x | Diagnosis = M)\\cdot P(texturemean = y | Diagnosis = M) = \\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{rM}}}e^{-\\frac{(x-\\mu_\\text{rM})^2}{2\\sigma_\\text{rM}^2}}\\right)\\cdot\\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{tM}}}e^{-\\frac{(y-\\mu_\\text{tM})^2}{2\\sigma_\\text{tM}^2}}\\right)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)\n",
    "\n",
    "\\begin{equation}\n",
    "P(Diagnosis = B | radius mean = x) = P(radius mean = x | Diagnosis = B)\\cdot P(texture mean = y | Diagnosis = B)\\cdot P(Diagnosis = M)\n",
    "\\end{equation}\n",
    "\n",
    "Now in order to evaluate the likelihood probability **P(radiusmean = x | Diagnosis = B) . P(texturemean = y | Diagnosis = M)** which is given by **Multivariate Joint Gaussian  Distribution PDF**. Now, for this PDF, we need to find out *the best estimate of the parameters of Normal Distribution* because we are assuming that our malignant tumor training data is being sampled from a Normal (gaussian) Distribution. The two parameters will be namely: *mu_hat_b & sigma_hat_b*\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "P(radiusmean = x | Diagnosis = B)\\cdot P(texturemean = y | Diagnosis = B) = \\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{rB}}}e^{-\\frac{(x-\\mu_\\text{rB})^2}{2\\sigma_\\text{rB}^2}}\\right)\\cdot\\left(\\frac{1}{\\sqrt{2\\pi}\\hat{\\sigma_\\text{tB}}}e^{-\\frac{(y-\\mu_\\text{tB})^2}{2\\sigma_\\text{tB}^2}}\\right)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.stats as s\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaussian_nb:\n",
    "    \n",
    "    \"\"\"Instantiate a Gaussian Naive Bayes Object with the following parameters:\n",
    "    \n",
    "    features            : A dataframe consisting of continuous features, exluding labels\n",
    "    labels              : A series consisting of binary labels\n",
    "    data_split_ratio    : A tuple consisting of data splitting ratio\n",
    "    apply_pca           : Boolean value specifying whether to apply PCA or not\n",
    "    n_components        : Number of Eigen Vectors having Non Zero values to keep\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(xerox_copy,features,labels,data_split_ratio,apply_pca,n_components):\n",
    "        \n",
    "        xerox_copy.binary_labels = np.array(labels).reshape(labels.shape[0],1)     \n",
    "        \n",
    "        xerox_copy.split_ratio = data_split_ratio\n",
    "        \n",
    "        xerox_copy.n_principal_components = n_components\n",
    "        \n",
    "        xerox_copy.unique_labels = list(labels.unique()) #We are doing Monkey Patching \n",
    "        \n",
    "        if apply_pca == True:\n",
    "            \n",
    "            xerox_copy.X_new = xerox_copy.apply_dim_reduction(features,xerox_copy.n_principal_components) #We are doing Monkey Patching\n",
    "            \n",
    "            \n",
    "            \n",
    "    def apply_dim_reduction(xerox_copy,data,n_components):\n",
    "        \n",
    "        X = np.array(data)\n",
    "        \n",
    "        mu = np.mean(X,axis=0)\n",
    "        \n",
    "        mu = mu.reshape(-1,mu.shape[0])\n",
    "        \n",
    "        X_dash = X - mu\n",
    "        \n",
    "        sigma_hat = (1/data.shape[0])*np.matmul(X_dash.T,X_dash)\n",
    "        \n",
    "        sigma_hat_decompose = np.linalg.svd(sigma_hat)\n",
    "        \n",
    "        Q = sigma_hat_decompose[0]\n",
    "        \n",
    "        Q_tilda = Q[:,0:n_components]\n",
    "        \n",
    "        X_new = np.matmul(X_dash,Q_tilda)\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    \n",
    "    \n",
    "    def data_splitting(xerox_copy):\n",
    "        \n",
    "        new_data = pd.DataFrame(data=xerox_copy.X_new)\n",
    "        \n",
    "        new_data['label'] = xerox_copy.binary_labels\n",
    "        \n",
    "        training_data_len = int(xerox_copy.split_ratio[0]*new_data.shape[0])\n",
    "        \n",
    "        neg_training_data = new_data[new_data['label'] == xerox_copy.unique_labels[0]].iloc[0:training_data_len//2]\n",
    "\n",
    "        pos_training_data = new_data[new_data['label'] == xerox_copy.unique_labels[1]].iloc[0:training_data_len//2]\n",
    "        \n",
    "        training_data = pd.concat([neg_training_data,pos_training_data])\n",
    "        \n",
    "        cv_data_len = int(xerox_copy.split_ratio[1]*new_data.shape[0])\n",
    "        \n",
    "        neg_remaining_data = new_data[new_data['label'] == xerox_copy.unique_labels[0]].iloc[training_data_len//2:]\n",
    "\n",
    "        pos_remaining_data = new_data[new_data['label'] == xerox_copy.unique_labels[1]].iloc[training_data_len//2:]\n",
    "        \n",
    "        remaining_data = pd.concat([neg_remaining_data,pos_remaining_data])\n",
    "        \n",
    "        cv_data = remaining_data.iloc[0:cv_data_len]\n",
    "\n",
    "        testing_data = remaining_data.iloc[cv_data_len:]\n",
    "        \n",
    "        return training_data,cv_data,testing_data\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_gaussian_nb(xerox_copy,data):\n",
    "        \n",
    "        mu_hat_pos = np.array(data[data['label'] == xerox_copy.unique_labels[1]].iloc[:,0:xerox_copy.n_principal_components].mean())\n",
    "\n",
    "        sigma_hat_pos = np.array(data[data['label'] == xerox_copy.unique_labels[1]].iloc[:,0:xerox_copy.n_principal_components].cov())\n",
    "        \n",
    "        mu_hat_neg = np.array(data[data['label'] == xerox_copy.unique_labels[0]].iloc[:,0:xerox_copy.n_principal_components].mean())\n",
    "\n",
    "        sigma_hat_neg = np.array(data[data['label'] == xerox_copy.unique_labels[0]].iloc[:,0:xerox_copy.n_principal_components].cov())\n",
    "        \n",
    "        xerox_copy.neg_likelihood_params = (mu_hat_neg,sigma_hat_neg) #We are using Monkey Patching here\n",
    "        \n",
    "        xerox_copy.pos_likelihood_params = (mu_hat_pos,sigma_hat_pos) #We are using Monkey Patching here\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def evaluate(xerox_copy,data):\n",
    "    \n",
    "        inputs = np.array(data.iloc[:,0:xerox_copy.n_principal_components])\n",
    "    \n",
    "        posterior_pos = s.multivariate_normal.pdf(inputs,xerox_copy.pos_likelihood_params[0],xerox_copy.pos_likelihood_params[1]) \n",
    "    \n",
    "        posterior_neg = s.multivariate_normal.pdf(inputs,xerox_copy.neg_likelihood_params[0],xerox_copy.neg_likelihood_params[1])\n",
    "    \n",
    "        boolean_mask = posterior_pos > posterior_neg\n",
    "    \n",
    "        predicted_category = pd.Series(boolean_mask)\n",
    "    \n",
    "        predicted_category.replace(to_replace=[False,True],value=[xerox_copy.unique_labels[0],xerox_copy.unique_labels[1]],inplace=True)\n",
    "    \n",
    "        predicted_results = np.array(predicted_category)\n",
    "        \n",
    "        actual_results = np.array(data['label'])\n",
    "        \n",
    "        print(classification_report(actual_results,predicted_results,target_names=xerox_copy.unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to run a Module as a Script\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Going to run a Module as a Script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
